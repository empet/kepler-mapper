{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeplerMapper & NLP examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newsgroups20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kmapper import jupyter\n",
    "import kmapper as km\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We will use the Newsgroups20 dataset. This is a canonical NLP dataset containing 11314 labeled postings on 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "X, y, target_names = np.array(newsgroups.data), np.array(newsgroups.target), np.array(newsgroups.target_names)\n",
    "print(\"SAMPLE\",X[0])\n",
    "print(\"SHAPE\",X.shape)\n",
    "print(\"TARGET\",target_names[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection\n",
    "To project the unstructured text dataset down to 2 fixed dimensions, we will set up a function pipeline. Every consecutive function will take as input the output from the previous function.\n",
    "\n",
    "We will try out \"Latent Semantic Char-Gram Analysis followed by Isometric Mapping\".\n",
    "\n",
    "- TFIDF vectorize (1-6)-chargrams and discard the top 17% and bottom 5% chargrams. Dimensionality = 13967.\n",
    "- Run TruncatedSVD with 100 components on this representation. TFIDF followed by Singular Value Decomposition is called Latent Semantic Analysis. Dimensionality = 100.\n",
    "- Run Isomap embedding on the output from previous step to project down to 2 dimensions. Dimensionality = 2.\n",
    "- MinMaxScale the output from previous step. Dimensionality = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(X,\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,6),\n",
    "                                max_df=0.83,\n",
    "                                min_df=0.05),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(\"SHAPE\",projected_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "We cover the projection with 10 33%-overlapping intervals per dimension (10\\*10=100 cubes total).\n",
    "\n",
    "We cluster on the projection (but, note, we can also create an `inverse_X` to cluster on by vectorizing the original text data).\n",
    "\n",
    "For clustering we use Agglomerative Single Linkage Clustering with the \"cosine\"-distance and 3 clusters. Agglomerative Clustering is a good cluster algorithm for TDA, since it both creates pleasing informative networks, and it has strong theoretical garantuees (see [functor](https://en.wikipedia.org/wiki/Functor) and [functoriality](https://jeremykun.com/2013/07/14/functoriality/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "graph = mapper.map(projected_X,\n",
    "                   inverse_X=None,\n",
    "                   clusterer=cluster.AgglomerativeClustering(n_clusters=3,\n",
    "                                                             linkage=\"complete\",\n",
    "                                                             affinity=\"cosine\"),\n",
    "                   overlap_perc=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable inverse X\n",
    "Here we show the flexibility of KeplerMapper by creating an `interpretable_inverse_X` that is easier to interpret by humans.\n",
    "\n",
    "For text, this can be TFIDF (1-3)-wordgrams, like we do here. For structured data this can be regularitory/protected variables of interest, or using another model to select, say, the top 10% features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.97,\n",
    "                      min_df=0.02)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "print(\"FEATURE NAMES SAMPLE\", interpretable_inverse_X_names[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "We use `interpretable_inverse_X` as the `inverse_X` during visualization. This way we get cluster statistics that are more informative/interpretable to humans (chargrams vs. wordgrams).\n",
    "\n",
    "We also pass the `projected_X` to get cluster statistics for the projection. For `custom_tooltips` we use a textual description of the label.\n",
    "\n",
    "The color function is simply the multi-class ground truth represented as a non-negative integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "html = mapper.visualize(graph,\n",
    "                        inverse_X=interpretable_inverse_X,\n",
    "                        inverse_X_names=interpretable_inverse_X_names,\n",
    "                        path_html=\"newsgroups20.html\",\n",
    "                        projected_X=projected_X,\n",
    "                        projected_X_names=[\"ISOMAP1\", \"ISOMAP2\"],\n",
    "                        title=\"Newsgroups20: Latent Semantic Char-gram Analysis with Isometric Embedding\",\n",
    "                        custom_tooltips=np.array([target_names[ys] for ys in y]),\n",
    "                        color_function=y)\n",
    "# jupyter.display(\"newsgroups20.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://i.imgur.com/3G4sm4Y.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
